# ğŸš€ GuÃ­a RÃ¡pida - Correcciones del Modelo Predictivo

## Â¿QuÃ© se ha corregido?

Se han implementado **10 mejoras crÃ­ticas** al sistema de calibraciÃ³n de sensores, enfocadas en:

1. âœ… **Robustez** - Manejo de outliers y valores extremos
2. âœ… **ValidaciÃ³n** - Cross-validation y detecciÃ³n de overfitting
3. âœ… **PrecisiÃ³n** - RegularizaciÃ³n y optimizaciÃ³n de hiperparÃ¡metros
4. âœ… **Confiabilidad** - MÃ©tricas adicionales (RÂ² ajustado, CV)
5. âœ… **DiagnÃ³stico** - Herramientas de anÃ¡lisis automÃ¡tico

---

## ğŸƒ Inicio RÃ¡pido (5 minutos)

### 1. Verificar instalaciÃ³n
```bash
cd "C:\Proyecto Maestria 23 Sep\fase 3"
python -c "from modules.calibration import get_calibration_models; print('âœ… OK')"
```

### 2. Ejecutar diagnÃ³stico
```bash
python test_model_corrections.py
```

### 3. Ver comparaciÃ³n detallada
```bash
python compare_models.py
```

### 4. Probar con tu cÃ³digo existente
```python
# Tu cÃ³digo existente sigue funcionando igual
from modules.calibration import train_and_evaluate_models

result = train_and_evaluate_models(lowcost_df, rmcab_df, pollutant='pm25')

# Ahora con informaciÃ³n adicional:
print(f"Outliers eliminados: {result['outliers_removed']}")
print(f"Mejor modelo: {result['best_model']}")

for model in result['results']:
    print(f"{model['model_name']}:")
    print(f"  RÂ² ajustado: {model['r2_adjusted']}")
    print(f"  Overfitting: {model['overfitting']['message']}")
```

---

## ğŸ“‹ Principales Correcciones

### 1. **DetecciÃ³n de Overfitting** ğŸ”
```python
# Ahora detecta automÃ¡ticamente si un modelo estÃ¡ sobreajustado
{
  'overfitting': {
    'status': 'ok',  # o 'overfitting'
    'severity': 'none',  # 'none', 'moderate', 'high'
    'message': 'No se detectÃ³ overfitting'
  }
}
```

**Â¿Por quÃ© es importante?**
- Evita modelos que "memorizan" datos de entrenamiento
- Identifica problemas antes del deployment
- Mejora la confiabilidad de las predicciones

---

### 2. **ValidaciÃ³n Cruzada (Cross-Validation)** ğŸ¯
```python
# EvaluaciÃ³n mÃ¡s robusta con 5 folds
{
  'cv_r2_mean': 0.9156,  # RÂ² promedio en 5 folds
  'cv_r2_std': 0.0234     # DesviaciÃ³n estÃ¡ndar (variabilidad)
}
```

**Â¿Por quÃ© es importante?**
- EvaluaciÃ³n mÃ¡s confiable del modelo
- Detecta si el rendimiento es estable
- Reduce sesgo en la estimaciÃ³n de mÃ©tricas

---

### 3. **Manejo de Outliers** ğŸ“Š
```python
# Elimina automÃ¡ticamente valores atÃ­picos
result = train_and_evaluate_models(
    lowcost_df, 
    rmcab_df,
    remove_outliers_flag=True  # NUEVO
)

print(f"Registros originales: {result['records']}")
print(f"Registros limpios: {result['records_after_cleaning']}")
print(f"Outliers eliminados: {result['outliers_removed']}")
```

**Â¿Por quÃ© es importante?**
- Outliers pueden distorsionar el entrenamiento
- Mejora la generalizaciÃ³n del modelo
- Predicciones mÃ¡s estables

---

### 4. **RÂ² Ajustado** ğŸ“
```python
# MÃ©trica mÃ¡s justa que penaliza modelos complejos
{
  'r2': 0.9245,           # RÂ² tradicional
  'r2_adjusted': 0.9198   # RÂ² ajustado (mÃ¡s realista)
}
```

**Â¿Por quÃ© es importante?**
- RÂ² simple puede ser engaÃ±oso con muchas variables
- RÂ² ajustado penaliza la complejidad innecesaria
- Mejor para comparar modelos con diferentes features

---

### 5. **RegularizaciÃ³n Mejorada** ğŸ›¡ï¸
```python
# Nuevos modelos con regularizaciÃ³n
models = {
    'Ridge Regression': Ridge(alpha=1.0),  # NUEVO
    'Random Forest': RandomForestRegressor(
        max_depth=15,           # Limita profundidad
        min_samples_split=5,    # RegularizaciÃ³n
        min_samples_leaf=2      # RegularizaciÃ³n
    ),
    'SVR (RBF)': SVR(C=10.0, epsilon=0.1)  # Optimizado
}
```

**Â¿Por quÃ© es importante?**
- Reduce overfitting
- Mejora generalizaciÃ³n
- Modelos mÃ¡s estables

---

### 6. **RobustScaler** ğŸ’ª
```python
# MÃ¡s robusto ante outliers que StandardScaler
result = train_and_evaluate_models(
    lowcost_df,
    rmcab_df,
    use_robust_scaler=True  # NUEVO
)
```

**ComparaciÃ³n:**
| Scaler | Centro | Escala | Robustez |
|--------|--------|--------|----------|
| StandardScaler | Media | Std Dev | Baja |
| RobustScaler | Mediana | IQR | Alta âœ… |

---

## ğŸ“Š Ejemplo Completo

```python
from modules.data_loader import load_lowcost_data, load_rmcab_data
from modules.calibration import train_and_evaluate_models

# 1. Cargar datos
lowcost_df = load_lowcost_data(
    start_date='2025-06-01',
    end_date='2025-07-31',
    devices=['Aire2']
)

rmcab_df = load_rmcab_data(
    station_code=6,  # Las Ferias
    start_date='2025-06-01',
    end_date='2025-07-31'
)

# 2. Calibrar con todas las mejoras
result = train_and_evaluate_models(
    lowcost_df,
    rmcab_df,
    pollutant='pm25',
    test_size=0.25,
    device_name='Aire2',
    remove_outliers_flag=True,    # Eliminar outliers
    use_robust_scaler=True        # Usar RobustScaler
)

# 3. Analizar resultados
if result['error']:
    print(f"âŒ Error: {result['error']}")
else:
    print(f"âœ… CalibraciÃ³n exitosa!")
    print(f"\nğŸ“Š Resumen de datos:")
    print(f"   Registros originales: {result['records']}")
    print(f"   Registros limpios: {result['records_after_cleaning']}")
    print(f"   Outliers eliminados: {result['outliers_removed']}")
    print(f"\nğŸ† Mejor modelo: {result['best_model']}")
    
    print(f"\nğŸ“ˆ Resultados por modelo:")
    for model in result['results']:
        print(f"\n   {model['model_name']}:")
        print(f"      RÂ²: {model['r2']:.4f}")
        print(f"      RÂ² ajustado: {model['r2_adjusted']:.4f}")
        
        if 'cv_r2_mean' in model:
            print(f"      CV RÂ² (Â±std): {model['cv_r2_mean']:.4f} Â± {model['cv_r2_std']:.4f}")
        
        print(f"      RMSE: {model['rmse']:.2f}")
        print(f"      MAE: {model['mae']:.2f}")
        print(f"      MAPE: {model['mape']:.2f}%")
        
        overfitting = model['overfitting']
        status_emoji = "âœ…" if overfitting['status'] == 'ok' else "âš ï¸"
        print(f"      Overfitting: {status_emoji} {overfitting['message']}")
```

---

## ğŸ¯ Casos de Uso

### Caso 1: CalibraciÃ³n BÃ¡sica (sin cambios en tu cÃ³digo)
```python
# Tu cÃ³digo existente funciona igual
result = train_and_evaluate_models(lowcost_df, rmcab_df)
# Ahora con mÃ¡s informaciÃ³n automÃ¡ticamente
```

### Caso 2: CalibraciÃ³n con Limpieza de Datos
```python
result = train_and_evaluate_models(
    lowcost_df,
    rmcab_df,
    remove_outliers_flag=True  # Activa limpieza
)

print(f"Se eliminaron {result['outliers_removed']} outliers")
```

### Caso 3: CalibraciÃ³n Robusta (para datos ruidosos)
```python
result = train_and_evaluate_models(
    lowcost_df,
    rmcab_df,
    remove_outliers_flag=True,
    use_robust_scaler=True  # MÃ¡s robusto
)
```

### Caso 4: AnÃ¡lisis de Overfitting
```python
result = train_and_evaluate_models(lowcost_df, rmcab_df)

for model in result['results']:
    overfitting = model['overfitting']
    if overfitting['status'] == 'overfitting':
        print(f"âš ï¸ {model['model_name']}: {overfitting['message']}")
```

---

## ğŸ” Scripts de DiagnÃ³stico

### test_model_corrections.py
Verifica que todas las correcciones funcionen correctamente:
- âœ… MAPE con valores cero
- âœ… NormalizaciÃ³n correcta
- âœ… DetecciÃ³n de overfitting
- âœ… IdentificaciÃ³n de outliers
- âœ… ValidaciÃ³n cruzada

```bash
python test_model_corrections.py
```

### compare_models.py
Genera reporte comparativo completo:
```bash
python compare_models.py
```

---

## â“ FAQ - Preguntas Frecuentes

### Â¿Es retrocompatible?
âœ… **SÃ­**, tu cÃ³digo existente funciona sin cambios. Las mejoras son automÃ¡ticas.

### Â¿Puedo desactivar la eliminaciÃ³n de outliers?
âœ… **SÃ­**, usa `remove_outliers_flag=False` (por defecto es True).

### Â¿CuÃ¡ntos datos necesito como mÃ­nimo?
ğŸ“Š MÃ­nimo **60 registros** despuÃ©s del merge y limpieza.

### Â¿La validaciÃ³n cruzada es obligatoria?
âš¡ No, solo se ejecuta si hay **â‰¥100 registros** en entrenamiento.

### Â¿QuÃ© pasa si tengo pocos datos?
âœ… El sistema lo detecta y ajusta automÃ¡ticamente (sin CV si <100 registros).

### Â¿CÃ³mo sÃ© si hay overfitting?
ğŸ” Revisa `model['overfitting']['status']` en los resultados.

### Â¿QuÃ© modelo debo usar?
ğŸ† El sistema selecciona automÃ¡ticamente el mejor (menor RMSE).

---

## ğŸ“š DocumentaciÃ³n Adicional

- **MEJORAS_MODELO_PREDICTIVO.md** - DocumentaciÃ³n tÃ©cnica completa
- **test_model_corrections.py** - Script de diagnÃ³stico
- **compare_models.py** - Reporte de comparaciÃ³n
- **modules/calibration.py** - CÃ³digo fuente con comentarios

---

## ğŸ“ Para tu Tesis

### Nuevos Puntos a Destacar

1. **ValidaciÃ³n Cruzada**
   - "Se implementÃ³ validaciÃ³n cruzada K-Fold (k=5) para evaluar de forma mÃ¡s robusta el rendimiento de los modelos"

2. **DetecciÃ³n de Overfitting**
   - "Se desarrollÃ³ un sistema automÃ¡tico de detecciÃ³n de sobreajuste basado en la diferencia de mÃ©tricas entre entrenamiento y test"

3. **Manejo de Outliers**
   - "Se implementaron dos mÃ©todos de detecciÃ³n de valores atÃ­picos: IQR (Interquartile Range) y Z-Score"

4. **RegularizaciÃ³n**
   - "Se aplicaron tÃ©cnicas de regularizaciÃ³n (Ridge L2, limitaciÃ³n de profundidad en Random Forest) para mejorar la generalizaciÃ³n"

5. **MÃ©tricas Robustas**
   - "Se incluyÃ³ RÂ² ajustado que penaliza la complejidad del modelo, proporcionando una evaluaciÃ³n mÃ¡s justa"

---

## âœ… Checklist de ImplementaciÃ³n

Antes de usar en producciÃ³n:

- [x] âœ… CÃ³digo implementado y probado
- [x] âœ… DocumentaciÃ³n completa
- [x] âœ… Scripts de diagnÃ³stico
- [ ] â³ Pruebas con datos reales
- [ ] â³ ComparaciÃ³n con versiÃ³n anterior
- [ ] â³ ActualizaciÃ³n del frontend
- [ ] â³ ValidaciÃ³n con expertos

---

## ğŸš€ PrÃ³ximos Pasos

1. **Ejecutar diagnÃ³stico**
   ```bash
   python test_model_corrections.py
   ```

2. **Ver comparaciÃ³n**
   ```bash
   python compare_models.py
   ```

3. **Probar con tus datos**
   ```python
   # Tu cÃ³digo aquÃ­
   ```

4. **Revisar documentaciÃ³n tÃ©cnica**
   - Leer: MEJORAS_MODELO_PREDICTIVO.md

5. **Integrar con frontend** (si es necesario)
   - Mostrar nuevas mÃ©tricas en la UI
   - Visualizar informaciÃ³n de overfitting
   - Mostrar outliers eliminados

---

## ğŸ’¡ Soporte

Â¿Problemas o dudas?

1. Revisa **MEJORAS_MODELO_PREDICTIVO.md**
2. Ejecuta **test_model_corrections.py**
3. Consulta los comentarios en **calibration.py**

---

**Â¡Tu modelo predictivo ahora es mÃ¡s robusto, confiable y cientÃ­ficamente riguroso! ğŸ‰**

Fecha: 5 de noviembre de 2025  
VersiÃ³n: 2.0
